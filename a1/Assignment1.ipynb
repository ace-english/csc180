{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Yelp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import relevent libraries. Add useful functions from the \"useful gems\" lab.I have also included a helper function to tell me that my browser is not crashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import time, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from scipy.stats import zscore\n",
    "import sklearn.feature_extraction.text as sk_text\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from collections.abc import Sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import winsound\n",
    "\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column. \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if isinstance(target_type, Sequence) else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)\n",
    "    \n",
    "#Show progress bar in loop\n",
    "def update_progress(progress):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "    block = int(round(bar_length * progress))\n",
    "\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)\n",
    "    \n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "#Let me know when you're done!\n",
    "def beep():\n",
    "    duration = 500  # milliseconds\n",
    "    freq = 1040  # Hz\n",
    "    winsound.Beep(freq, duration)\n",
    "\n",
    "loadFresh=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First determine businesses have enough reviews to be relevent for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(loadFresh):\n",
    "    #Convert raw business data into tsv\n",
    "    outfile = open(\"businesses.tsv\", 'w')\n",
    "    sfile = csv.writer(outfile, delimiter =\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
    "    sfile.writerow(['business_id','stars', 'review_count'])\n",
    "    with open('yelp_dataset/business.json', encoding=\"utf-8\") as f:\n",
    "     i=0\n",
    "     for line in f:\n",
    "        row = json.loads(line)\n",
    "        if(row['review_count']>=20):\n",
    "            sfile.writerow([row['business_id'], row['stars'], (row['review_count'])])\n",
    "        i=i+1\n",
    "        update_progress(i / 192610)\n",
    "\n",
    "    update_progress(1);\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3041</td>\n",
       "      <td>--I7YYLada0tSLkORTHb5Q</td>\n",
       "      <td>3.5</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4225</td>\n",
       "      <td>-092wE7j5HZOogMLAh40zA</td>\n",
       "      <td>3.0</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4572</td>\n",
       "      <td>-1VaIJza42Hjev6ukacCNg</td>\n",
       "      <td>4.0</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>883</td>\n",
       "      <td>-1xuC540Nycht_iWFeJ-dw</td>\n",
       "      <td>4.5</td>\n",
       "      <td>465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>-4-MzST67P_jnX4mh3MIcw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4297</td>\n",
       "      <td>zyPGYeXF4XKCqNN1pjFWhg</td>\n",
       "      <td>4.5</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1076</td>\n",
       "      <td>zyr5pzOs3SJIX3K-nvC2zg</td>\n",
       "      <td>4.5</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3274</td>\n",
       "      <td>zz3CqZhNx2rQ_Yp6zHze-A</td>\n",
       "      <td>3.0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2265</td>\n",
       "      <td>zzOo9n22fBbKAhbSpMzggA</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>zzwhN7x37nyjP0ZM8oiHmw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6300 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id  stars  review_count\n",
       "3041  --I7YYLada0tSLkORTHb5Q    3.5            96\n",
       "4225  -092wE7j5HZOogMLAh40zA    3.0            92\n",
       "4572  -1VaIJza42Hjev6ukacCNg    4.0           210\n",
       "883   -1xuC540Nycht_iWFeJ-dw    4.5           465\n",
       "1080  -4-MzST67P_jnX4mh3MIcw    4.0            20\n",
       "...                      ...    ...           ...\n",
       "4297  zyPGYeXF4XKCqNN1pjFWhg    4.5           163\n",
       "1076  zyr5pzOs3SJIX3K-nvC2zg    4.5            74\n",
       "3274  zz3CqZhNx2rQ_Yp6zHze-A    3.0            47\n",
       "2265  zzOo9n22fBbKAhbSpMzggA    3.0            25\n",
       "356   zzwhN7x37nyjP0ZM8oiHmw    4.0            54\n",
       "\n",
       "[6300 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load business table\n",
    "business_df= pd.read_csv('businesses.tsv', delimiter =\"\\t\", encoding=\"utf-8\")\n",
    "business_df=business_df.sort_values('business_id');\n",
    "business_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found 57644 with enough reviews to be useful. All stored in business_df, ordered by business ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review data\n",
    "Open the review data and convert into a TSV with only the relevent information. Runs a progress bar while loading. To save space, only writes to file if the business has 20 reviews or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(loadFresh):\n",
    "    #Convert raw review data into TSV\n",
    "    outfile = open(\"review_stars.tsv\", 'w')\n",
    "    sfile = csv.writer(outfile, delimiter =\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
    "    sfile.writerow(['business_id','stars', 'text'])\n",
    "    with open('yelp_dataset/review.json', encoding=\"utf-8\") as f:\n",
    "     i=0\n",
    "     for line in f:\n",
    "        row = json.loads(line)\n",
    "        # Check if businessID if present in business table\n",
    "        if(row['business_id'] in business_df['business_id'].tolist()):\n",
    "            # some special char must be encoded in 'utf-8'\n",
    "            sfile.writerow([row['business_id'], row['stars'], (row['text']).encode('utf-8')])\n",
    "        update_progress(i / 2000000)\n",
    "        i=i+1\n",
    "\n",
    "    update_progress(1);\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ujmEBvifdJM6h6RLv4wQIg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'Total bill for this horrible service? Over $...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>NZnhc2sEQy3RmzKTZnqtwQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>b\"I *adore* Travis at the Hard Rock's new Kell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
       "      <td>5.0</td>\n",
       "      <td>b\"I have to say that this office really has it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>b1b1eb3uo-w561D0ZfCEiQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'Today was my second out of three sessions I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>eU_713ec6fTGNO4BegRaww</td>\n",
       "      <td>4.0</td>\n",
       "      <td>b'I\\'ll be the first to admit that I was not e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2232051</td>\n",
       "      <td>BLvcA14BVJYSPjaVnNUKFg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>b'Delicious locally roasted organic  coffee. F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2232052</td>\n",
       "      <td>gCElF94aCMjB2SeFOEHLXg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>b\"I recently visited this location with my fam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2232053</td>\n",
       "      <td>oIFMGuV8C8yeaDrtwjc8Jw</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'This place is a rip off! Charge you 44 if yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2232054</td>\n",
       "      <td>9M4bfZJlTSXfzGoKnQzdww</td>\n",
       "      <td>5.0</td>\n",
       "      <td>b'We recently needed to have our patio roof re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2232055</td>\n",
       "      <td>w0Zivn7ajMsUYbcQ2759uw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>b'We came here for the first time a couple of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2232056 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    business_id  stars  \\\n",
       "0        ujmEBvifdJM6h6RLv4wQIg    1.0   \n",
       "1        NZnhc2sEQy3RmzKTZnqtwQ    5.0   \n",
       "2        WTqjgwHlXbSFevF32_DJVw    5.0   \n",
       "3        b1b1eb3uo-w561D0ZfCEiQ    1.0   \n",
       "4        eU_713ec6fTGNO4BegRaww    4.0   \n",
       "...                         ...    ...   \n",
       "2232051  BLvcA14BVJYSPjaVnNUKFg    5.0   \n",
       "2232052  gCElF94aCMjB2SeFOEHLXg    5.0   \n",
       "2232053  oIFMGuV8C8yeaDrtwjc8Jw    1.0   \n",
       "2232054  9M4bfZJlTSXfzGoKnQzdww    5.0   \n",
       "2232055  w0Zivn7ajMsUYbcQ2759uw    4.0   \n",
       "\n",
       "                                                      text  \n",
       "0        b'Total bill for this horrible service? Over $...  \n",
       "1        b\"I *adore* Travis at the Hard Rock's new Kell...  \n",
       "2        b\"I have to say that this office really has it...  \n",
       "3        b'Today was my second out of three sessions I ...  \n",
       "4        b'I\\'ll be the first to admit that I was not e...  \n",
       "...                                                    ...  \n",
       "2232051  b'Delicious locally roasted organic  coffee. F...  \n",
       "2232052  b\"I recently visited this location with my fam...  \n",
       "2232053  b'This place is a rip off! Charge you 44 if yo...  \n",
       "2232054  b'We recently needed to have our patio roof re...  \n",
       "2232055  b'We came here for the first time a couple of ...  \n",
       "\n",
       "[2232056 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load review data\n",
    "df= pd.read_csv('review_stars.tsv', delimiter =\"\\t\", encoding=\"utf-8\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language preprocessing\n",
    "First, aggregates all reviews under their business ID. Then we can run TFIDF on the sum of all the reviews in preparation to do language analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ac895c8072a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mcol_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_reviews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'business_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#create dataframe from vectored reviews\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mdf_vector_reviews\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_reviews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;31m#concat to single dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mdf_tfidf_reviews\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_vector_reviews\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36mtodense\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mshares\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m         \"\"\"\n\u001b[1;32m--> 848\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1024\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1025\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1184\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#groups reviews by businessID\n",
    "df_review_group=df.groupby('business_id')['text'].sum()\n",
    "df_reviews = pd.DataFrame({'business_id':df_review_group.index, 'all_reviews':df_review_group.values})\n",
    "df_reviews\n",
    "\n",
    "#language preprocessing\n",
    "vectorizer = sk_text.TfidfVectorizer(stop_words='english',\n",
    "                             min_df=2, \n",
    "                             max_df=500)\n",
    "\n",
    "#df_reviews['all_reviews'] = vectorizer.fit_transform(df_reviews['all_reviews'])\n",
    "vector_reviews=vectorizer.fit_transform(df_reviews[\"all_reviews\"])\n",
    "\n",
    "#split off business ID\n",
    "col_id=df_reviews['business_id']\n",
    "#create dataframe from vectored reviews\n",
    "df_vector_reviews=pd.DataFrame(vector_reviews.todense())\n",
    "#concat to single dataframe\n",
    "df_tfidf_reviews=pd.concat([col_id,df_vector_reviews],axis=1)\n",
    "\n",
    "display(df_tfidf_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join total reviews with aggregated review data\n",
    "df_join=pd.concat([df, df_tfidf_reviews], axis=1, join='inner')\n",
    "#drop unnecesary info\n",
    "df_sklearn_ready=df_join.drop(columns=['business_id', 'text'])\n",
    "df_sklearn_ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y is our list of reviews and X is your table of language data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get arrays for training\n",
    "y=df_sklearn_ready[\"stars\"].values\n",
    "x=df_sklearn_ready.drop([\"stars\"], axis=1).values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is clean and it's time to feed it to the AI\n",
    "\n",
    "# Machine learning\n",
    "## TensorFlow Regression\n",
    "Since the idea is to make a bunch of models to test, to start here are some functions to repeatedly create and test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to make model\n",
    "def make_model(x,y,firstLayer=25,secondLayer=10,epochs=50, optimizer='adam', activation='relu',\n",
    "               stopEarly=1, monitor='loss', min_delta=0.001, patience=2, verbose=2):\n",
    "    model = Sequential()\n",
    "\n",
    "    #Set up layers\n",
    "    model.add(Dense(25, input_dim=x.shape[1], activation=activation)) # Hidden 1 \n",
    "    model.add(Dense(10, activation)) # Hidden 2\n",
    "    model.add(Dense(1)) # Output\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "    #set up early stop parameters\n",
    "    if(stopEarly):\n",
    "        earlyStop = EarlyStopping(monitor=monitor, min_delta=min_delta, patience=patience, verbose=verbose, mode='auto')  \n",
    "        model.fit(x, y, verbose=verbose, epochs=epochs, callbacks=[earlyStop])\n",
    "    else:\n",
    "        model.fit(x, y, verbose=verbose, epochs=epochs)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with RSME Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to test a bunch of models and return the best one\n",
    "def test_models(models, x, y):\n",
    "    #used to return best model, but used too much memory\n",
    "    best_i=-1\n",
    "    i=0\n",
    "    best_rmse=5;\n",
    "    for model in models:\n",
    "        pred=model.predict(x)\n",
    "        #compare actual to predicted\n",
    "        score = np.sqrt(metrics.mean_squared_error(pred,y))\n",
    "        print(score)\n",
    "        if(score<best_rmse):\n",
    "            best_i=i\n",
    "            best_rmse=score\n",
    "        i=i+1\n",
    "    return models[best_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create models with different control variables, several times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "Now we will creatie many models with different starting points, attributes, and algorithms.\n",
    "\n",
    "First we will test adam vs. sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slower sg models\n",
    "sgd_models=[]\n",
    "for i in range(3):\n",
    "    sgd_models.append(make_model(x_train,y_train, optimizer='sgd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_final=test_models(sgd_models,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#control\n",
    "control_models=[]\n",
    "for i in range(4):\n",
    "    control_models.append(make_model(x_train,y_train, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_final=test_models(control_models,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD is much more effective, so future tests will use it.\n",
    "\n",
    "Next we will compare activation algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid models\n",
    "sigmoid_models=[]\n",
    "for i in range(3):\n",
    "    sigmoid_models.append(make_model(x_train,y_train, optimizer='sgd', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_final=test_models(sigmoid_models,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tanh models\n",
    "tanh_models=[]\n",
    "for i in range(3):\n",
    "    tanh_models.append(make_model(x_train,y_train, optimizer='sgd', activation='tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh_final=test_models(tanh_models,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid is the better activation to use, so we will stick with that.\n",
    "\n",
    "Next we will play with number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more epochs\n",
    "slow_models=[]\n",
    "for i in range(3):\n",
    "    slow_models.append(make_model(x_train,y_train, epochs=100, optimizer='sgd', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_final=test_models(slow_models,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#way more epochs\n",
    "x_slow_models=[]\n",
    "for i in range(3):\n",
    "    x_slow_models.append(make_model(x_train,y_train, epochs=200, optimizer='sgd', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_slow_final=test_models(x_slow_models,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to early stopping, number of epochs is negligible on results\n",
    "\n",
    "Next we will play with layer sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more neurons\n",
    "big_models=[]\n",
    "for i in range(3):\n",
    "    big_models.append(make_model(x_train,y_train, firstLayer=50, secondLayer=25, optimizer='sgd', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_final=test_models(big_models,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fewer neurons\n",
    "small_models=[]\n",
    "for i in range(3):\n",
    "    small_models.append(make_model(x_train,y_train, firstLayer=10, secondLayer=5, optimizer='sgd', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "small_final=test_models(small_models,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More neurons helped seems a little better. What if we used *a lot* more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#way more neurons\n",
    "xl_models=[]\n",
    "for i in range(3):\n",
    "    xl_models.append(make_model(x_train,y_train, firstLayer=100, secondLayer=50, optimizer='sgd', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl_final=test_models(xl_models,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of neurons produced a negligable difference in result.\n",
    "\n",
    "Next let us experiment with number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parfait_models=[]\n",
    "for i in range(3):\n",
    "    model = Sequential()\n",
    "\n",
    "    #Set up layers\n",
    "    model.add(Dense(25, input_dim=x.shape[1], activation='sigmoid')) # Hidden 1 \n",
    "    model.add(Dense(20, 'sigmoid')) # Hidden 2\n",
    "    model.add(Dense(10, 'sigmoid')) # Hidden 3\n",
    "    model.add(Dense(1)) # Output\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "    #set up early stop parameters\n",
    "    earlyStop = EarlyStopping(monitor='loss', min_delta=0.001, patience=2, verbose=0, mode='auto')  \n",
    "    model.fit(x_train, y_train, verbose=0, epochs=50, callbacks=[earlyStop])\n",
    "    parfait_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parfait_final=test_models(parfait_models,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onion_models=[]\n",
    "for i in range(3):\n",
    "    model = Sequential()\n",
    "\n",
    "    #Set up layers\n",
    "    model.add(Dense(50, input_dim=x.shape[1], activation='sigmoid')) # Hidden 1 \n",
    "    model.add(Dense(25, 'sigmoid')) # Hidden 2\n",
    "    model.add(Dense(15, 'sigmoid')) # Hidden 3\n",
    "    model.add(Dense(1)) # Output\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "    #set up early stop parameters\n",
    "    earlyStop = EarlyStopping(monitor='loss', min_delta=0.001, patience=2, verbose=0, mode='auto')  \n",
    "    model.fit(x, y, verbose=0, epochs=50, callbacks=[earlyStop])\n",
    "    onion_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onion_final=test_models(onion_models,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of layers seems negiligible with the current settings.\n",
    "\n",
    "Next let's play with the min delta and pateince variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#higher patience value\n",
    "patient_models=[]\n",
    "for i in range(3):\n",
    "    patient_models.append(make_model(x_train,y_train, patience=5, optimizer='sgd', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_final=test_models(patient_models,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower pateince value\n",
    "impatient_models=[]\n",
    "for i in range(3):\n",
    "    impatient_models.append(make_model(x_train,y_train,  patience=1, optimizer='sgd', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impatient_final=test_models(impatient_models,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#smaller min delta value\n",
    "ten_k_models=[]\n",
    "for i in range(3):\n",
    "    ten_k_models.append(make_model(x_train,y_train, min_delta=0.0001, optimizer='sgd', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_k_final=test_models(ten_k_models,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#larger min delta value\n",
    "hun_models=[]\n",
    "for i in range(3):\n",
    "    hun_models.append(make_model(x_train,y_train, min_delta=0.01, patience=5, optimizer='sgd', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hun_final=test_models(hun_models,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaller min delta value AND more patient\n",
    "patient_10k_models=[]\n",
    "for i in range(3):\n",
    "    patient_10k_models.append(make_model(x_train,y_train, min_delta=0.0001, patience=5, optimizer='sgd', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_10k_final=test_models(patient_10k_models,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've picked the best configuration, let's run it from multiple starting points to get the best possible model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models=[]\n",
    "for i in range(10):\n",
    "    best_models.append(make_model(x_train,y_train, verbose=0, firstLayer=100, secondLayer=50, \n",
    "                                min_delta=0.0001, optimizer='sgd', activation='sigmoid'))\n",
    "best_model=test_models(best_models,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to make model\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#Set up layers\n",
    "model.add(Dense(25, input_dim=x.shape[1], activation=activation)) # Hidden 1 \n",
    "model.add(Dense(10, activation)) # Hidden 2\n",
    "model.add(Dense(1)) # Output\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "  #set up early stop parameters\n",
    "earlyStop = EarlyStopping(monitor=monitor, min_delta=min_delta, patience=patience, verbose=verbose, mode='auto') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=make_model(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(x_test)\n",
    "chart_regression(pred.flatten(),y_test,sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# save entire network to HDF5\n",
    "best_model.save(\"network.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model2 = load_model(\"network.hdf5\")\n",
    "pred = model2.predict(x_test)\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"After load score (RMSE): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
